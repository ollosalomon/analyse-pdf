# Interface utilisateur Streamlit
import streamlit as st
import os
import time
import tempfile
from datetime import datetime
import pandas as pd
import plotly.express as px
import json
import sys
import requests
sys.path.append(".")
import logging
logger = logging.getLogger("chatbot")
logging.basicConfig(level=logging.INFO)

# Configuration de la page
st.set_page_config(
    page_title="PDF Analyzer - Traitement de PDF volumineux",
    page_icon="üìÑ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS de style
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #424242;
        margin-bottom: 1rem;
    }
    .info-box, .success-box, .warning-box, .error-box {
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
    }
    .info-box { background-color: #E3F2FD; }
    .success-box { background-color: #E8F5E9; }
    .warning-box { background-color: #FFF8E1; }
    .error-box { background-color: #FFEBEE; }
</style>
""", unsafe_allow_html=True)


@st.cache_resource
def get_temp_dir():
    return tempfile.mkdtemp()

def check_ollama_status():
    """V√©rifier le statut d'Ollama et des mod√®les"""
    try:
        # V√©rifier si Ollama est accessible
        response = requests.get("http://ollama:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            model_names = [model['name'] for model in models]
            return True, model_names
        else:
            return False, []
    except Exception:
        return False, []


def init_session_state():
    if 'processing_complete' not in st.session_state:
        st.session_state.processing_complete = False
    if 'report_generated' not in st.session_state:
        st.session_state.report_generated = False
    if 'pdf_path' not in st.session_state:
        st.session_state.pdf_path = None
    if 'output_dir' not in st.session_state:
        st.session_state.output_dir = "output"
    if 'processing_stats' not in st.session_state:
        st.session_state.processing_stats = None
    if 'report_path' not in st.session_state:
        st.session_state.report_path = None
    if 'current_page' not in st.session_state:
        st.session_state.current_page = "upload"
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'ollama_status' not in st.session_state:
        st.session_state.ollama_status = None
    # NOUVEAUX √âTATS √Ä AJOUTER
    if 'uploaded_file' not in st.session_state:
        st.session_state.uploaded_file = None
    if 'batch_size' not in st.session_state:
        st.session_state.batch_size = 20
    if 'report_name' not in st.session_state:
        st.session_state.report_name = "rapport_final.md"
    if 'file_uploaded' not in st.session_state:
        st.session_state.file_uploaded = False

def show_ollama_status():
    """Afficher le statut d'Ollama dans la sidebar"""
    with st.sidebar:
        st.markdown("### ü§ñ Statut Ollama")
        
        if st.button("üîÑ V√©rifier Ollama"):
            st.session_state.ollama_status = None
        
        if st.session_state.ollama_status is None:
            with st.spinner("V√©rification..."):
                ollama_ok, models = check_ollama_status()
                st.session_state.ollama_status = (ollama_ok, models)
        
        ollama_ok, models = st.session_state.ollama_status
        
        if ollama_ok:
            st.success("‚úÖ Ollama connect√©")
            if models:
                st.info(f"üì¶ Mod√®les: {', '.join(models)}")
            else:
                st.warning("‚ö†Ô∏è Aucun mod√®le install√©")
        else:
            st.error("‚ùå Ollama non accessible")

def page_upload():
    st.markdown('<h1 class="main-header">Traitement de PDF volumineux</h1>', unsafe_allow_html=True)
    st.markdown("""
        <div style="background-color: #1f77b4; color: white; padding: 15px; border-radius: 10px; font-size: 16px;">
        Cet outil vous permet de traiter des fichiers PDF volumineux (jusqu'√† 2000 pages), 
        d'extraire leur contenu structur√© et de g√©n√©rer un rapport d'analyse.
        </div>
        """, unsafe_allow_html=True)

    # MODIFI√â : Maintenir l'√©tat du fichier upload√©
    if st.session_state.file_uploaded and st.session_state.uploaded_file:
        st.success(f"üìÑ Fichier d√©j√† charg√©: {st.session_state.uploaded_file.name}")
        
        # Bouton pour changer de fichier
        if st.button("üîÑ Changer de fichier"):
            st.session_state.uploaded_file = None
            st.session_state.file_uploaded = False
            st.rerun()
    
    # Upload du fichier seulement si pas d√©j√† upload√©
    if not st.session_state.file_uploaded:
        uploaded_file = st.file_uploader("Choisissez un fichier PDF", type=["pdf"])
        if uploaded_file:
            st.session_state.uploaded_file = uploaded_file
            st.session_state.file_uploaded = True
            st.success(f"üìÑ Fichier charg√©: {uploaded_file.name}")
            st.rerun()

    col1, col2 = st.columns(2)

    with col1:
        # MODIFI√â : Utiliser l'√©tat de session
        batch_size = st.slider("Taille des lots", 10, 100, st.session_state.batch_size, key="batch_slider")
        st.session_state.batch_size = batch_size

    with col2:
        # MODIFI√â : Utiliser l'√©tat de session
        report_name = st.text_input("Nom du rapport", value=st.session_state.report_name, key="report_input")
        st.session_state.report_name = report_name

    # MODIFI√â : Traitement seulement si fichier upload√©
    if st.session_state.file_uploaded and st.session_state.uploaded_file:
        # Ne pas sauvegarder le PDF sur le disque
        st.session_state.pdf_path = None

        if st.button("Lancer le traitement", type="primary"):
            # V√©rifications pr√©alables
            ollama_ok, models = check_ollama_status()
            if not ollama_ok:
                st.error("‚ùå Ollama n'est pas accessible. V√©rifiez que le service est d√©marr√©.")
                return

            # Utiliser le buffer du fichier pour le traitement
            try:
                file_size = st.session_state.uploaded_file.size
                st.info(f"üìä Taille du fichier: {file_size / (1024*1024):.1f} MB")
            except Exception as e:
                st.error(f"‚ùå Impossible de lire le fichier: {str(e)}")
                return

            # V√©rifier les imports n√©cessaires
            try:
                st.info("üîç V√©rification des d√©pendances...")
                from app.interface.cli import process_pdf_in_batches, generate_report_from_pdf
                st.success("‚úÖ Modules import√©s avec succ√®s")
            except ImportError as e:
                st.error(f"‚ùå Erreur d'import critique: {str(e)}")
                st.info("V√©rifiez que tous les modules sont pr√©sents dans le conteneur")
                return
            except Exception as e:
                st.error(f"‚ùå Erreur lors de l'import: {str(e)}")
                return

            # Traitement avec gestion d'erreurs renforc√©e
            with st.spinner("Traitement du PDF en cours..."):
                progress_bar = st.progress(0)
                status_text = st.empty()

                try:
                    status_text.text("üîÑ Initialisation du traitement...")
                    progress_bar.progress(10)
                    time.sleep(0.5)

                    status_text.text("üìñ Lecture du PDF...")
                    progress_bar.progress(25)

                    # Appel de la fonction avec le buffer du fichier
                    st.info("üöÄ Lancement du traitement par lots...")
                    stats = process_pdf_in_batches(st.session_state.uploaded_file, st.session_state.output_dir, batch_size)

                    progress_bar.progress(100)
                    status_text.text("‚úÖ Traitement termin√©!")

                    st.session_state.processing_stats = stats
                    st.session_state.processing_complete = True
                    st.success("üéâ Traitement termin√© avec succ√®s!")
                    st.balloons()
                    
                except FileNotFoundError as e:
                    st.error(f"‚ùå Fichier non trouv√©: {str(e)}")
                    st.info("V√©rifiez que le fichier PDF est valide")
                    
                except MemoryError as e:
                    st.error("‚ùå Erreur de m√©moire: Fichier trop volumineux")
                    st.info("Essayez avec une taille de lot plus petite ou un fichier plus petit")
                    
                except ImportError as e:
                    st.error(f"‚ùå Module manquant: {str(e)}")
                    st.info("V√©rifiez l'installation des d√©pendances")
                    
                except Exception as e:
                    st.error(f"‚ùå Erreur lors du traitement: {str(e)}")
                    st.error(f"Type d'erreur: {type(e).__name__}")
                    
                    # Affichage d√©taill√© de l'erreur
                    with st.expander("üêõ D√©tails de l'erreur (pour le debug)"):
                        import traceback
                        st.code(traceback.format_exc())
                    
                    # Informations de debug
                    st.info("üîß Informations de debug:")
                    st.write(f"- Chemin ou nom du fichier: {getattr(uploaded_file, 'name', str(uploaded_file))}")
                    st.write(f"- Dossier de sortie: {st.session_state.output_dir}")
                    st.write(f"- Taille du lot: {batch_size}")
                    st.write(f"- Python path: {sys.path[:3]}...")
                    st.write(f"- Ollama status: {ollama_ok}")
                    
                    return
                    
                finally:
                    progress_bar.empty()
                    status_text.empty()

        # Section g√©n√©ration du rapport (inchang√©e)
        if st.session_state.processing_complete:
            st.markdown("---")
            st.markdown('<h2 class="sub-header">üìä G√©n√©ration du rapport</h2>', unsafe_allow_html=True)

            if st.button("G√©n√©rer le rapport", type="secondary"):
                with st.spinner("G√©n√©ration du rapport..."):
                    try:
                        # MODIFI√â : Utiliser les valeurs de session
                        path = generate_report_from_pdf(
                            st.session_state.uploaded_file, 
                            st.session_state.output_dir, 
                            st.session_state.batch_size, 
                            st.session_state.report_name
                        )
                        st.session_state.report_path = path
                        st.session_state.report_generated = True
                        st.success("üìÑ Rapport g√©n√©r√© avec succ√®s!")
                        st.session_state.current_page = "results"
                        st.rerun()
                    except Exception as e:
                        st.error(f"‚ùå Erreur lors de la g√©n√©ration du rapport: {str(e)}")
                            
def page_results():
    st.markdown('<h1 class="main-header">üìä R√©sultats du traitement</h1>', unsafe_allow_html=True)

    if not st.session_state.processing_complete:
        st.warning("‚ö†Ô∏è Aucun traitement effectu√©.")
        if st.button("Retour √† l'upload"):
            st.session_state.current_page = "upload"
            st.rerun()
        return

    st.markdown('<h2 class="sub-header">üìà Statistiques de traitement</h2>', unsafe_allow_html=True)
    stats = st.session_state.processing_stats

    if stats:
        # V√©rifier si stats est un dict ou un tuple
        if isinstance(stats, dict):
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("üìÑ Pages trait√©es", stats.get("pages_processed", 0))
            col2.metric("üñºÔ∏è Images extraites", stats.get("images_extracted", 0))
            col3.metric("üìù Chunks cr√©√©s", stats.get("chunks_created", 0))
            col4.metric("‚è±Ô∏è Dur√©e", f"{stats.get('processing_time_seconds', 0) / 60:.1f} min")
        else:
            # Si stats est un tuple ou autre format, affichage basique
            st.info(f"üìä Statistiques: {stats}")
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("üìÑ Pages trait√©es", "N/A")
            col2.metric("üñºÔ∏è Images extraites", "N/A")
            col3.metric("üìù Chunks cr√©√©s", "N/A")
            col4.metric("‚è±Ô∏è Dur√©e", "N/A")
            
        # Graphique de distribution
        total_pages = stats.get("total_pages", 100)
        if total_pages > 0:
            pages = list(range(1, min(total_pages + 1, 200), max(1, total_pages // 20)))
            chunks = [20 + (i * 3) % 15 for i in range(len(pages))]
            df = pd.DataFrame({"Page": pages, "Chunks": chunks})
            fig = px.bar(df, x="Page", y="Chunks", 
                        title="Distribution des chunks par page",
                        color="Chunks",
                        color_continuous_scale="viridis")
            st.plotly_chart(fig, use_container_width=True)

    if st.session_state.report_generated and st.session_state.report_path:
        st.markdown('<h2 class="sub-header">üìÑ Rapport g√©n√©r√©</h2>', unsafe_allow_html=True)
        try:
            with open(st.session_state.report_path, "r", encoding="utf-8") as f:
                report_content = f.read()
            
            # Afficher un aper√ßu du rapport
            with st.expander("üëÅÔ∏è Aper√ßu du rapport", expanded=True):
                st.markdown(report_content[:2000] + ("..." if len(report_content) > 2000 else ""))

            # Bouton de t√©l√©chargement
            with open(st.session_state.report_path, "rb") as f:
                st.download_button(
                    "üì• T√©l√©charger le rapport complet", 
                    f.read(), 
                    "rapport_analyse.md", 
                    mime="text/markdown",
                    type="primary"
                )
        except Exception as e:
            st.error(f"‚ùå Erreur lors de la lecture du rapport : {str(e)}")

    if st.button("üîÑ Traiter un nouveau PDF"):
        # MODIFI√â : R√©initialisation plus propre sans affecter la navigation
        keys_to_reset = [
            "processing_complete", "report_generated", "pdf_path", 
            "processing_stats", "report_path", "uploaded_file", 
            "file_uploaded"
        ]
        for key in keys_to_reset:
            if key in st.session_state:
                if key in ["pdf_path", "processing_stats", "report_path", "uploaded_file"]:
                    st.session_state[key] = None
                elif key == "file_uploaded":
                    st.session_state[key] = False
                else:
                    st.session_state[key] = False
        
        # R√©initialiser les valeurs par d√©faut
        st.session_state.batch_size = 20
        st.session_state.report_name = "rapport_final.md"
        
        st.session_state.current_page = "upload" 
        st.rerun()

@st.cache_resource
def get_vector_db(vector_db_path):
    """Cache la base vectorielle pour √©viter de la recharger"""
    from app.vectorstore.chroma_db import EnhancedVectorDB
    from app.vectorstore.embeddings import get_embedding_model
    embedding_model = get_embedding_model()
    vector_db = EnhancedVectorDB(
        embedding_model, 
        persist_directory=vector_db_path
    )
    return vector_db

@st.cache_resource
def get_llm():
    """Cache le mod√®le LLM"""
    from langchain_community.llms import Ollama
    return Ollama(
        model="llava",
        base_url="http://ollama:11434",
        temperature=0.7,
        timeout=60
    )

@st.cache_resource
def get_vision_llm():
    """LLM pour la vision (LLaVA) - utilis√© pour l'analyse d'images"""
    from langchain_community.llms import Ollama
    return Ollama(
        model="llava",
        base_url="http://ollama:11434",
        temperature=0.7,
        timeout=60,  # Plus long pour la vision
        num_ctx=4096  # Contexte plus large pour les images
    )

@st.cache_resource
def get_text_llm():
    """LLM l√©ger pour le texte uniquement - chatbot"""
    from langchain_community.llms import Ollama
    return Ollama(
        model="phi3:mini",  # Mod√®le ultra-l√©ger
        base_url="http://ollama:11434",
        temperature=0.3,
        timeout=10,  # Timeout court
        num_predict=300,  # R√©ponses concises
        num_ctx=2048  # Contexte r√©duit
    )

@st.cache_resource  
def get_qa_chain(_vector_db):
    """Cha√Æne QA optimis√©e avec le mod√®le l√©ger"""
    if _vector_db is None:
        return None
        
    from langchain.chains import RetrievalQA
    from langchain.prompts import PromptTemplate
    
    # Template optimis√© pour les r√©ponses rapides
    prompt_template = """Bas√© sur le contexte suivant, r√©ponds de mani√®re concise et pr√©cise.
    Si l'information n'est pas dans le contexte, dis-le clairement.

    Contexte: {context}

    Question: {question}

    R√©ponse (max 3 phrases):"""
    
    PROMPT = PromptTemplate(
        template=prompt_template, 
        input_variables=["context", "question"]
    )
    
    # Utiliser le mod√®le l√©ger pour le texte
    llm = get_text_llm()
    
    # Retriever optimis√©
    retriever = _vector_db.get_retriever()
    retriever.search_kwargs = {"k": 2}  # Seulement 2 documents
    
    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )

def check_available_models():
    """V√©rifier quels mod√®les sont disponibles"""
    try:
        import requests
        response = requests.get("http://ollama:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            model_names = [model['name'] for model in models]
            return model_names
        return []
    except:
        return []

def get_optimal_text_model():
    """Choisir le meilleur mod√®le disponible pour le texte"""
    available_models = check_available_models()
    
    # Ordre de pr√©f√©rence (du plus l√©ger au plus lourd)
    preferred_models = [
        "phi3:mini",
        "phi3:3.8b", 
        "gemma:2b",
        "llama3.2:3b",
        "mistral:7b",
        "llama2:7b",
        "llava"  # Fallback
    ]
    
    for model in preferred_models:
        if any(model in available for available in available_models):
            return model
    
    return "llava"  # Fallback par d√©faut

@st.cache_resource
def get_adaptive_text_llm():
    """LLM adaptatif qui choisit le meilleur mod√®le disponible"""
    from langchain_community.llms import Ollama
    
    optimal_model = get_optimal_text_model()
    
    # Param√®tres selon le mod√®le
    if "phi3" in optimal_model or "gemma" in optimal_model:
        # Mod√®les ultra-l√©gers
        params = {
            "temperature": 0.2,
            "timeout": 8,
            "num_predict": 200,
            "num_ctx": 2048
        }
    elif "llama3.2" in optimal_model or "mistral" in optimal_model:
        # Mod√®les moyens
        params = {
            "temperature": 0.3,
            "timeout": 15,
            "num_predict": 250,
            "num_ctx": 2048
        }
    else:
        # Fallback (llava, etc.)
        params = {
            "temperature": 0.5,
            "timeout": 20,
            "num_predict": 200,
            "num_ctx": 2048
        }
    
    st.info(f"ü§ñ Utilisation du mod√®le: {optimal_model}")
    
    return Ollama(
        model=optimal_model,
        base_url="http://ollama:11434",
        **params
    )

@st.cache_resource  
def get_qa_chain_adaptive(_vector_db, timeout=60):
    """Cr√©e une cha√Æne QA adaptative avec timeout configur√©"""
    try:
        from langchain_community.llms import Ollama
        from langchain.chains import RetrievalQA
        
        # Configuration du LLM avec timeout
        optimal_model = get_optimal_text_model()
        
        # Param√®tres optimis√©s selon le mod√®le
        if optimal_model == "phi3:mini":
            llm_params = {
                "model": optimal_model,
                "base_url": "http://ollama:11434",
                "timeout": timeout,
                "temperature": 0.3,
                "top_p": 0.9,
                "num_ctx": 2048,  # Contexte r√©duit pour plus de rapidit√©
                "repeat_penalty": 1.1
            }
        else:  # llava ou autres
            llm_params = {
                "model": optimal_model,
                "base_url": "http://ollama:11434",
                "timeout": timeout,
                "temperature": 0.5,
                "top_p": 0.8,
                "num_ctx": 4096,
                "repeat_penalty": 1.2
            }
        
        llm = Ollama(**llm_params)
        
        # Cr√©ation de la cha√Æne QA
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            # retriever=_vector_db.get_retriever(
            #     search_kwargs={"k": 3}  # Limite √† 3 r√©sultats pour plus de rapidit√©
            # ),
            retriever=_vector_db.get_retriever(),
            return_source_documents=False
        )
        
        return qa_chain
        
    except Exception as e:
        st.error(f"Erreur lors de la cr√©ation de la cha√Æne QA: {e}")
        return None
    
    
def check_ollama_connection():
    """V√©rifie si Ollama est accessible"""
    try:
        import requests
        response = requests.get("http://ollama:11434/api/tags", timeout=5)
        return response.status_code == 200
    except:
        return False

def determine_pdf_context_needed(user_input, conversation_mode):
    """D√©termine si la question n√©cessite le contexte PDF"""
    
    if conversation_mode == "üí≠ Conversation libre":
        return False
    elif conversation_mode == "üìÑ Analyse PDF":
        return True
    else:  # Mode hybride
        # Mots-cl√©s indiquant une question sur le document
        pdf_keywords = [
            "document", "pdf", "texte", "page", "section", "chapitre",
            "que dit", "selon le", "dans le document", "extrait",
            "r√©sum√©", "analyse", "contenu", "information"
        ]
        
        user_lower = user_input.lower()
        return any(keyword in user_lower for keyword in pdf_keywords)

def handle_pdf_question(user_input, vector_db_path, timeout):
    """Traite une question n√©cessitant le contexte PDF"""
    
    # R√©ponses rapides
    quick_responses = {
        "r√©sum√©": "üìã Voici un r√©sum√© du document...",
        "sommaire": "üìë Table des mati√®res du document...",
    }
    
    user_lower = user_input.lower().strip()
    if user_lower in quick_responses:
        return quick_responses[user_lower]
    
    # RAG standard
    vector_db = get_vector_db(vector_db_path)
    if not vector_db:
        return "‚ùå Base vectorielle inaccessible"
    
    qa_chain = get_qa_chain_adaptive(vector_db, timeout=timeout)
    if not qa_chain:
        return "‚ùå Impossible de cr√©er la cha√Æne QA"
    
    result = qa_chain.invoke({"query": user_input})
    return result["result"]

def handle_general_question(user_input, timeout):
    """Traite une question g√©n√©rale sans contexte PDF"""
    
    # R√©ponses rapides pour les salutations
    quick_responses = {
        "bonjour": "üëã Bonjour ! Je suis votre assistant IA. Comment puis-je vous aider ?",
        "salut": "üëã Salut ! Que puis-je faire pour vous aujourd'hui ?",
        "hello": "üëã Hello! How can I help you today?",
        "merci": "üòä De rien ! Autre chose ?",
        "qui es-tu": "ü§ñ Je suis un assistant IA polyvalent, capable d'analyser des PDF et de discuter de tout sujet !",
        "que peux-tu faire": "üîß Je peux analyser des PDF, r√©pondre √† vos questions, vous aider dans vos t√¢ches et discuter de nombreux sujets !"
    }
    
    user_lower = user_input.lower().strip()
    if user_lower in quick_responses:
        return quick_responses[user_lower]
    
    # Conversation g√©n√©rale avec Ollama
    try:
        from langchain_community.llms import Ollama
        
        optimal_model = get_optimal_text_model()
        
        # Configuration pour conversation g√©n√©rale
        llm_params = {
            "model": optimal_model,
            "base_url": "http://ollama:11434",
            "timeout": timeout,
            "temperature": 0.7,  # Plus cr√©atif pour conversation g√©n√©rale
            "top_p": 0.9,
            "num_ctx": 2048 if optimal_model == "phi3:mini" else 4096,
            "repeat_penalty": 1.1
        }
        
        llm = Ollama(**llm_params)
        
        # Prompt pour conversation g√©n√©rale
        prompt = f"""Tu es un assistant IA serviable et amical. R√©ponds √† cette question de mani√®re utile et conversationnelle :

Question: {user_input}

R√©ponse:"""
        
        response = llm.invoke(prompt)
        return response
        
    except Exception as e:
        return f"‚ùå Erreur lors de la g√©n√©ration: {str(e)}"

def display_response_time(processing_time):
    """Affiche le temps de r√©ponse avec couleurs adaptatives"""
    if processing_time < 5:
        st.success(f"‚ö° R√©ponse ultra-rapide ({processing_time:.1f}s)")
    elif processing_time < 15:
        st.success(f"‚úÖ R√©ponse rapide ({processing_time:.1f}s)")
    elif processing_time < 30:
        st.warning(f"‚è±Ô∏è R√©ponse normale ({processing_time:.1f}s)")
    else:
        st.info(f"üïê R√©ponse lente ({processing_time:.1f}s)")

def handle_chat_error(error, processing_time, timeout):
    """G√®re les erreurs de chat avec messages sp√©cifiques"""
    error_msg = str(error)
    
    if "timeout" in error_msg.lower() or "read timed out" in error_msg.lower():
        st.error(f"‚è±Ô∏è Timeout apr√®s {processing_time:.1f}s")
        st.info("üí° Essayez avec un timeout plus long ou une question plus simple")
    elif "connection" in error_msg.lower():
        st.error(f"üîå Probl√®me de connexion apr√®s {processing_time:.1f}s")
        st.info("üîß V√©rifiez que Ollama est d√©marr√©")
    else:
        st.error(f"‚ùå Erreur apr√®s {processing_time:.1f}s: {error_msg}")

def display_chat_history():
    """Affiche l'historique avec indication du mode utilis√©"""
    if st.session_state.chat_history:
        st.markdown("---")
        st.markdown("### üí¨ Historique")
        
        col1, col2 = st.columns([1, 4])
        with col1:
            if st.button("üóëÔ∏è Effacer"):
                st.session_state.chat_history = []
                st.rerun()
        with col2:
            show_all = st.checkbox("Afficher tout l'historique", value=False)
        
        # Limitation de l'affichage
        history_to_show = st.session_state.chat_history if show_all else st.session_state.chat_history[-6:]
        
        for i, item in enumerate(reversed(history_to_show)):
            if len(item) == 2:  # Ancien format
                sender, msg = item
                mode_icon = ""
            else:  # Nouveau format avec mode
                sender, msg, mode_icon = item
            
            if sender == "Vous":
                st.markdown(f"**üë§ {sender}:** {msg}")
            else:
                st.markdown(f"**ü§ñ Assistant {mode_icon}:** {msg}")
            
            if i < len(history_to_show) - 1:
                st.markdown("---")
# Modifier la fonction page_chatbot

def page_chatbot():
    st.markdown('<h1 class="main-header">üí¨ Assistant IA Polyvalent</h1>', unsafe_allow_html=True)
    
    # D√©tection du mode (avec ou sans PDF)
    vector_db_path = os.path.join("output", "vector_db")
    has_pdf = st.session_state.processing_complete and os.path.exists(vector_db_path)
    
    # S√©lection du mode de conversation
    with st.sidebar:
        st.markdown("### üéØ Mode de conversation")
        
        if has_pdf:
            conversation_mode = st.radio(
                "Choisissez le mode :",
                ["üìÑ Analyse PDF", "üí≠ Conversation libre", "üîÄ Mode hybride"],
                index=0
            )
        else:
            conversation_mode = st.radio(
                "Choisissez le mode :",
                ["üí≠ Conversation libre", "üìÑ Analyse PDF (n√©cessite upload)"],
                index=0
            )
            
        # Informations sur le mode s√©lectionn√©
        if conversation_mode == "üìÑ Analyse PDF":
            if has_pdf:
                st.success("‚úÖ PDF charg√© - Questions sur le document")
            else:
                st.warning("‚ö†Ô∏è Chargez d'abord un PDF dans l'onglet Upload")
        elif conversation_mode == "üí≠ Conversation libre":
            st.info("üó®Ô∏è Discussion g√©n√©rale sans document")
        elif conversation_mode == "üîÄ Mode hybride":
            st.info("üîÑ Questions sur PDF + conversation g√©n√©rale")

    # Affichage du mod√®le utilis√©
    with st.sidebar:
        st.markdown("### ü§ñ Mod√®les")
        available_models = check_available_models()
        if available_models:
            optimal_model = get_optimal_text_model()
            st.success(f"üìù Texte: {optimal_model}")
            if "llava" in available_models:
                st.success("üëÅÔ∏è Vision: llava")
            
            # Indicateur de performance du mod√®le
            if optimal_model == "phi3:mini":
                st.info("‚ö° Mod√®le rapide et efficace")
            elif optimal_model == "llava":
                st.warning("üêå Mod√®le lent mais polyvalent")
        else:
            st.error("‚ùå Aucun mod√®le d√©tect√©")

    # Configuration des timeouts
    with st.sidebar:
        st.markdown("### ‚öôÔ∏è Configuration")
        timeout_option = st.selectbox(
            "Timeout de g√©n√©ration:",
            ["Rapide (30s)", "Normal (60s)", "Lent (120s)", "Tr√®s lent (300s)"],
            index=1
        )
        
        timeout_mapping = {
            "Rapide (30s)": 30,
            "Normal (60s)": 60,
            "Lent (120s)": 120,
            "Tr√®s lent (300s)": 300
        }
        selected_timeout = timeout_mapping[timeout_option]

    # V√©rification selon le mode
    if conversation_mode == "üìÑ Analyse PDF" and not has_pdf:
        st.warning("‚ö†Ô∏è Mode analyse PDF s√©lectionn√© mais aucun document charg√©.")
        st.info("üëÜ Chargez un PDF dans l'onglet Upload ou basculez en 'Conversation libre'")
        return

    # Interface de chat adapt√©e au mode
    if conversation_mode == "üìÑ Analyse PDF":
        placeholder_text = "üí≠ Posez une question sur votre document PDF..."
    elif conversation_mode == "üí≠ Conversation libre":
        placeholder_text = "üí≠ Posez n'importe quelle question..."
    else:  # Mode hybride
        placeholder_text = "üí≠ Question sur le PDF ou conversation g√©n√©rale..."

    user_input = st.chat_input(placeholder_text)

    if user_input: 
        start_time = time.time()
        
        try:
            # V√©rification de la connexion Ollama
            if not check_ollama_connection():
                st.error("‚ùå Impossible de se connecter √† Ollama")
                return
            
            with st.spinner("ü§ñ G√©n√©ration de la r√©ponse..."):
                
                # D√©termination du type de r√©ponse n√©cessaire
                needs_pdf_context = determine_pdf_context_needed(user_input, conversation_mode)
                
                if needs_pdf_context and has_pdf:
                    # Mode RAG avec PDF
                    response = handle_pdf_question(user_input, vector_db_path, selected_timeout)
                else:
                    # Mode conversation libre
                    response = handle_general_question(user_input, selected_timeout)
                
                processing_time = time.time() - start_time
                
                # Ajouter √† l'historique avec le mode utilis√©
                mode_icon = "üìÑ" if needs_pdf_context else "üí≠"
                st.session_state.chat_history.append(("Vous", user_input))
                st.session_state.chat_history.append(("Assistant", response, mode_icon))
                
                # Affichage du temps
                display_response_time(processing_time)
                
        except Exception as e:
            processing_time = time.time() - start_time
            handle_chat_error(e, processing_time, selected_timeout)

    # Historique avec indication du mode
    display_chat_history()
    
def main():
    init_session_state()
    
    # Afficher le statut d'Ollama
    show_ollama_status()
    
    # MODIFI√â : Navigation qui respecte l'√©tat de session
    current_menu = None
    if st.session_state.current_page == "upload":
        current_menu = "üì§ Upload"
    elif st.session_state.current_page == "results":
        current_menu = "üìä R√©sultats"
    elif st.session_state.current_page == "chatbot":
        current_menu = "ü§ñ Chatbot"
    
    # Utiliser l'index pour maintenir la s√©lection
    menu_options = ["üì§ Upload", "üìä R√©sultats", "ü§ñ Chatbot"]
    try:
        default_index = menu_options.index(current_menu) if current_menu else 0
    except ValueError:
        default_index = 0
    
    menu = st.sidebar.radio("üß≠ Navigation", menu_options, index=default_index)
    
    # MODIFI√â : Mettre √† jour l'√©tat seulement si changement
    if menu == "üì§ Upload" and st.session_state.current_page != "upload":
        st.session_state.current_page = "upload"
        st.rerun()
    elif menu == "üìä R√©sultats" and st.session_state.current_page != "results":
        st.session_state.current_page = "results"
        st.rerun()
    elif menu == "ü§ñ Chatbot" and st.session_state.current_page != "chatbot":
        st.session_state.current_page = "chatbot"
        st.rerun()

    # Affichage des pages (inchang√©)
    try:
        if st.session_state.current_page == "upload":
            page_upload()
        elif st.session_state.current_page == "results":
            page_results()
        elif st.session_state.current_page == "chatbot":
            page_chatbot()
    except Exception as e:
        st.error(f"‚ùå Erreur dans l'application : {str(e)}")
        st.info("üîÑ Rechargez la page pour recommencer.")
        if st.checkbox("üêõ Mode debug - Afficher les d√©tails"):
            st.exception(e)


if __name__ == "__main__":
    main()