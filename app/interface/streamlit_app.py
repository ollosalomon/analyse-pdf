# Interface utilisateur Streamlit
import streamlit as st
import os
import time
import tempfile
from datetime import datetime
import pandas as pd
import plotly.express as px
import json
import sys
import requests
sys.path.append(".")
import logging
logger = logging.getLogger("chatbot")
logging.basicConfig(level=logging.INFO)

# Configuration de la page
st.set_page_config(
    page_title="PDF Analyzer - Traitement de PDF volumineux",
    page_icon="üìÑ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS de style
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 1rem;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #424242;
        margin-bottom: 1rem;
    }
    .info-box, .success-box, .warning-box, .error-box {
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
    }
    .info-box { background-color: #E3F2FD; }
    .success-box { background-color: #E8F5E9; }
    .warning-box { background-color: #FFF8E1; }
    .error-box { background-color: #FFEBEE; }
</style>
""", unsafe_allow_html=True)


@st.cache_resource
def get_temp_dir():
    return tempfile.mkdtemp()

def check_ollama_status():
    """V√©rifier le statut d'Ollama et des mod√®les"""
    try:
        # V√©rifier si Ollama est accessible
        response = requests.get("http://ollama:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get('models', [])
            model_names = [model['name'] for model in models]
            return True, model_names
        else:
            return False, []
    except Exception:
        return False, []

def init_session_state():
    if 'processing_complete' not in st.session_state:
        st.session_state.processing_complete = False
    if 'report_generated' not in st.session_state:
        st.session_state.report_generated = False
    if 'pdf_path' not in st.session_state:
        st.session_state.pdf_path = None
    if 'output_dir' not in st.session_state:
        st.session_state.output_dir = "output"  # <-- Dossier persistant
    if 'processing_stats' not in st.session_state:
        st.session_state.processing_stats = None
    if 'report_path' not in st.session_state:
        st.session_state.report_path = None
    if 'current_page' not in st.session_state:
        st.session_state.current_page = "upload"
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'ollama_status' not in st.session_state:
        st.session_state.ollama_status = None

def show_ollama_status():
    """Afficher le statut d'Ollama dans la sidebar"""
    with st.sidebar:
        st.markdown("### ü§ñ Statut Ollama")
        
        if st.button("üîÑ V√©rifier Ollama"):
            st.session_state.ollama_status = None
        
        if st.session_state.ollama_status is None:
            with st.spinner("V√©rification..."):
                ollama_ok, models = check_ollama_status()
                st.session_state.ollama_status = (ollama_ok, models)
        
        ollama_ok, models = st.session_state.ollama_status
        
        if ollama_ok:
            st.success("‚úÖ Ollama connect√©")
            if models:
                st.info(f"üì¶ Mod√®les: {', '.join(models)}")
            else:
                st.warning("‚ö†Ô∏è Aucun mod√®le install√©")
        else:
            st.error("‚ùå Ollama non accessible")

def page_upload():
    st.markdown('<h1 class="main-header">Traitement de PDF volumineux</h1>', unsafe_allow_html=True)
    st.markdown("""
        <div style="background-color: #1f77b4; color: white; padding: 15px; border-radius: 10px; font-size: 16px;">
        Cet outil vous permet de traiter des fichiers PDF volumineux (jusqu'√† 2000 pages), 
        d'extraire leur contenu structur√© et de g√©n√©rer un rapport d'analyse.
        </div>
        """, unsafe_allow_html=True)

    uploaded_file = st.file_uploader("Choisissez un fichier PDF", type=["pdf"])
    col1, col2 = st.columns(2)

    with col1:
        batch_size = st.slider("Taille des lots", 10, 100, 20)

    with col2:
        report_name = st.text_input("Nom du rapport", value="rapport_final.md")

    if uploaded_file:
        # Ne pas sauvegarder le PDF sur le disque
        st.session_state.pdf_path = None  # Pas de chemin local

        st.success(f"üìÑ Fichier charg√©: {uploaded_file.name}")

        if st.button("Lancer le traitement", type="primary"):
            # V√©rifications pr√©alables
            ollama_ok, models = check_ollama_status()
            if not ollama_ok:
                st.error("‚ùå Ollama n'est pas accessible. V√©rifiez que le service est d√©marr√©.")
                return

            # Utiliser le buffer du fichier pour le traitement
            try:
                file_size = uploaded_file.size
                st.info(f"üìä Taille du fichier: {file_size / (1024*1024):.1f} MB")
            except Exception as e:
                st.error(f"‚ùå Impossible de lire le fichier: {str(e)}")
                return

            # V√©rifier les imports n√©cessaires
            try:
                st.info("üîç V√©rification des d√©pendances...")
                from app.interface.cli import process_pdf_in_batches, generate_report_from_pdf
                st.success("‚úÖ Modules import√©s avec succ√®s")
            except ImportError as e:
                st.error(f"‚ùå Erreur d'import critique: {str(e)}")
                st.info("V√©rifiez que tous les modules sont pr√©sents dans le conteneur")
                return
            except Exception as e:
                st.error(f"‚ùå Erreur lors de l'import: {str(e)}")
                return

            # Traitement avec gestion d'erreurs renforc√©e
            with st.spinner("Traitement du PDF en cours..."):
                progress_bar = st.progress(0)
                status_text = st.empty()

                try:
                    status_text.text("üîÑ Initialisation du traitement...")
                    progress_bar.progress(10)
                    time.sleep(0.5)

                    status_text.text("üìñ Lecture du PDF...")
                    progress_bar.progress(25)

                    # Appel de la fonction avec le buffer du fichier
                    st.info("üöÄ Lancement du traitement par lots...")
                    stats = process_pdf_in_batches(uploaded_file, st.session_state.output_dir, batch_size)

                    progress_bar.progress(100)
                    status_text.text("‚úÖ Traitement termin√©!")

                    st.session_state.processing_stats = stats
                    st.session_state.processing_complete = True
                    st.success("üéâ Traitement termin√© avec succ√®s!")
                    st.balloons()
                    
                except FileNotFoundError as e:
                    st.error(f"‚ùå Fichier non trouv√©: {str(e)}")
                    st.info("V√©rifiez que le fichier PDF est valide")
                    
                except MemoryError as e:
                    st.error("‚ùå Erreur de m√©moire: Fichier trop volumineux")
                    st.info("Essayez avec une taille de lot plus petite ou un fichier plus petit")
                    
                except ImportError as e:
                    st.error(f"‚ùå Module manquant: {str(e)}")
                    st.info("V√©rifiez l'installation des d√©pendances")
                    
                except Exception as e:
                    st.error(f"‚ùå Erreur lors du traitement: {str(e)}")
                    st.error(f"Type d'erreur: {type(e).__name__}")
                    
                    # Affichage d√©taill√© de l'erreur
                    with st.expander("üêõ D√©tails de l'erreur (pour le debug)"):
                        import traceback
                        st.code(traceback.format_exc())
                    
                    # Informations de debug
                    st.info("üîß Informations de debug:")
                    st.write(f"- Chemin ou nom du fichier: {getattr(uploaded_file, 'name', str(uploaded_file))}")
                    st.write(f"- Dossier de sortie: {st.session_state.output_dir}")
                    st.write(f"- Taille du lot: {batch_size}")
                    st.write(f"- Python path: {sys.path[:3]}...")
                    st.write(f"- Ollama status: {ollama_ok}")
                    
                    return
                    
                finally:
                    progress_bar.empty()
                    status_text.empty()

        # Section g√©n√©ration du rapport (inchang√©e)
        if st.session_state.processing_complete:
            st.markdown("---")
            st.markdown('<h2 class="sub-header">üìä G√©n√©ration du rapport</h2>', unsafe_allow_html=True)

            if st.button("G√©n√©rer le rapport", type="secondary"):
                with st.spinner("G√©n√©ration du rapport..."):
                    try:
                        path = generate_report_from_pdf(temp_file, st.session_state.output_dir, batch_size, report_name)
                        st.session_state.report_path = path
                        st.session_state.report_generated = True
                        st.success("üìÑ Rapport g√©n√©r√© avec succ√®s!")
                        st.session_state.current_page = "results"
                        st.rerun()
                    except Exception as e:
                        st.error(f"‚ùå Erreur lors de la g√©n√©ration du rapport: {str(e)}")
                        with st.expander("üêõ D√©tails de l'erreur"):
                            import traceback
                            st.code(traceback.format_exc())
                            
def page_results():
    st.markdown('<h1 class="main-header">üìä R√©sultats du traitement</h1>', unsafe_allow_html=True)

    if not st.session_state.processing_complete:
        st.warning("‚ö†Ô∏è Aucun traitement effectu√©.")
        if st.button("Retour √† l'upload"):
            st.session_state.current_page = "upload"
            st.rerun()
        return

    st.markdown('<h2 class="sub-header">üìà Statistiques de traitement</h2>', unsafe_allow_html=True)
    stats = st.session_state.processing_stats

    if stats:
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("üìÑ Pages trait√©es", stats.get("pages_processed", 0))
        col2.metric("üñºÔ∏è Images extraites", stats.get("images_extracted", 0))
        col3.metric("üìù Chunks cr√©√©s", stats.get("chunks_created", 0))
        col4.metric("‚è±Ô∏è Dur√©e", f"{stats.get('processing_time_seconds', 0) / 60:.1f} min")

        # Graphique de distribution
        total_pages = stats.get("total_pages", 100)
        if total_pages > 0:
            pages = list(range(1, min(total_pages + 1, 200), max(1, total_pages // 20)))
            chunks = [20 + (i * 3) % 15 for i in range(len(pages))]
            df = pd.DataFrame({"Page": pages, "Chunks": chunks})
            fig = px.bar(df, x="Page", y="Chunks", 
                        title="Distribution des chunks par page",
                        color="Chunks",
                        color_continuous_scale="viridis")
            st.plotly_chart(fig, use_container_width=True)

    if st.session_state.report_generated and st.session_state.report_path:
        st.markdown('<h2 class="sub-header">üìÑ Rapport g√©n√©r√©</h2>', unsafe_allow_html=True)
        try:
            with open(st.session_state.report_path, "r", encoding="utf-8") as f:
                report_content = f.read()
            
            # Afficher un aper√ßu du rapport
            with st.expander("üëÅÔ∏è Aper√ßu du rapport", expanded=True):
                st.markdown(report_content[:2000] + ("..." if len(report_content) > 2000 else ""))

            # Bouton de t√©l√©chargement
            with open(st.session_state.report_path, "rb") as f:
                st.download_button(
                    "üì• T√©l√©charger le rapport complet", 
                    f.read(), 
                    "rapport_analyse.md", 
                    mime="text/markdown",
                    type="primary"
                )
        except Exception as e:
            st.error(f"‚ùå Erreur lors de la lecture du rapport : {str(e)}")

    if st.button("üîÑ Traiter un nouveau PDF"):
        # R√©initialisation propre
        keys_to_reset = ["processing_complete", "report_generated", "pdf_path", 
                        "processing_stats", "report_path"]
        for key in keys_to_reset:
            if key in st.session_state:
                st.session_state[key] = None if key in ["pdf_path", "processing_stats", "report_path"] else False
        st.session_state.current_page = "upload"
        st.rerun()

# def page_chatbot():
#     st.markdown('<h1 class="main-header">üí¨ Assistant d\'analyse PDF</h1>', unsafe_allow_html=True)
    
#     # V√©rifier Ollama
#     ollama_ok, models = check_ollama_status()
#     if not ollama_ok:
#         st.error("‚ùå Ollama n'est pas accessible. V√©rifiez que le service est d√©marr√©.")
#         return
    
#     if "llava" not in " ".join(models).lower():
#         st.warning("‚ö†Ô∏è Le mod√®le LLaVA n'est pas install√©. Installez-le avec: `ollama pull llava`")
#         return

#     st.markdown("Posez vos questions sur les documents trait√©s. Le mod√®le LLaVA r√©pondra en se basant sur la base vectorielle construite.")

#     # V√©rifier si un PDF a √©t√© trait√©
#     if not st.session_state.processing_complete:
#         st.warning("‚ö†Ô∏è Veuillez d'abord traiter un PDF avant d'utiliser le chatbot.")
#         return

#     # Interface de chat
#     user_input = st.text_input("üí≠ Votre question :", key="chat_input", placeholder="Posez votre question sur le document...")

#     if user_input and st.button("üì§ Envoyer", type="primary"):
#         try:
#             # Import des modules n√©cessaires
#             try:
#                 from app.vectorstore.chroma_db import EnhancedVectorDB
#                 from app.vectorstore.embeddings import get_embedding_model
#                 from langchain.chains import RetrievalQA
#                 from langchain_community.llms import Ollama
#             except ImportError as e:
#                 st.error(f"‚ùå Erreur d'import : {str(e)}")
#                 st.info("V√©rifiez que tous les modules requis sont install√©s dans requirements.txt")
#                 return

#             # V√©rifier la base vectorielle
#             vector_db_path = os.path.join(st.session_state.output_dir, "vector_db")
#             if not os.path.exists(vector_db_path):
#                 st.error("‚ùå Base vectorielle non trouv√©e. Traitez d'abord un PDF.")
#                 return

#             with st.spinner("ü§ñ G√©n√©ration de la r√©ponse..."):
#                 # Charger la base vectorielle
#                 embedding_model = get_embedding_model()
#                 vector_db = EnhancedVectorDB(embedding_model, persist_directory=vector_db_path)
#                 retriever = vector_db.get_retriever()

#                 # Configurer Ollama
#                 llm = Ollama(
#                     model="llava", 
#                     base_url="http://ollama:11434",
#                     temperature=0.7
#                 )

#                 # Cr√©er la cha√Æne de QA
#                 qa_chain = RetrievalQA.from_chain_type(
#                     llm=llm, 
#                     retriever=retriever,
#                     return_source_documents=True
#                 )
                
#                 # G√©n√©rer la r√©ponse
#                 result = qa_chain({"query": user_input})
#                 response = result["result"]
#                 sources = result.get("source_documents", [])

#                 # Stocker dans l'historique
#                 st.session_state.chat_history.append(("Vous", user_input))
#                 st.session_state.chat_history.append(("LLaVA", response))
                
#                 # Afficher la r√©ponse
#                 st.success("‚úÖ R√©ponse g√©n√©r√©e!")
                
#                 # Redirection pour actualiser l'interface
#                 st.rerun()
                
#         except Exception as e:
#             st.error(f"‚ùå Erreur dans le chatbot : {str(e)}")
#             if st.checkbox("Afficher les d√©tails de l'erreur"):
#                 st.exception(e)

#     # Affichage de l'historique
#     if st.session_state.chat_history:
#         st.markdown("### üí¨ Historique des conversations")
#         for i, (sender, msg) in enumerate(reversed(st.session_state.chat_history[-10:])):  # Limiter √† 10 derniers messages
#             if sender == "Vous":
#                 st.markdown(f"**üë§ {sender}** : {msg}")
#             else:
#                 st.markdown(f"**ü§ñ {sender}** : {msg}")
            
#             if i < len(st.session_state.chat_history) - 1:
#                 st.markdown("---")

@st.cache_resource
def get_vector_db(vector_db_path):
    """Cache la base vectorielle pour √©viter de la recharger"""
    from app.vectorstore.chroma_db import EnhancedVectorDB
    from app.vectorstore.embeddings import get_embedding_model
    embedding_model = get_embedding_model()
    vector_db = EnhancedVectorDB(
        embedding_model, 
        persist_directory=vector_db_path
    )
    return vector_db

@st.cache_resource
def get_llm():
    """Cache le mod√®le LLM"""
    from langchain_community.llms import Ollama
    return Ollama(
        model="llava",
        base_url="http://ollama:11434",
        temperature=0.7,
        timeout=60
    )

@st.cache_resource
def get_qa_chain(_vector_db):  # Notez le underscore ici
    """Cache la cha√Æne QA"""
    from langchain.chains import RetrievalQA
    llm = get_llm()
    return RetrievalQA.from_chain_type(
        llm=llm,
        retriever=_vector_db.get_retriever(),
        return_source_documents=True
    )
# Modifier la fonction page_chatbot
def page_chatbot():
    st.markdown('<h1 class="main-header">üí¨ Assistant d\'analyse PDF</h1>', unsafe_allow_html=True)
    
    # V√©rifications initiales inchang√©es...
    
    vector_db_path = os.path.join("output", "vector_db")
    if not os.path.exists(vector_db_path):
        st.warning("‚ö†Ô∏è Aucune base vectorielle trouv√©e...")
        return

    user_input = st.text_input("üí≠ Votre question :", key="chat_input")

    if user_input and st.button("üì§ Envoyer", type="primary"):
        try:
            with st.spinner("ü§ñ G√©n√©ration de la r√©ponse..."):
                # Utiliser les versions cach√©es
                vector_db = get_vector_db(vector_db_path)
                qa_chain = get_qa_chain(vector_db)  # Plus d'erreur ici
                
                result = qa_chain.invoke({"query": user_input})
                response = result["result"]
                
                st.session_state.chat_history.append(("Vous", user_input))
                st.session_state.chat_history.append(("LLaVA", response))
                st.success("‚úÖ R√©ponse g√©n√©r√©e!")
        except Exception as e:
            st.error(f"‚ùå Erreur : {str(e)}")
            if st.checkbox("D√©tails"):
                st.exception(e)
    # Affichage de l'historique
    if st.session_state.chat_history:
        st.markdown("### üí¨ Historique des conversations")
        for i, (sender, msg) in enumerate(reversed(st.session_state.chat_history[-10:])):  # Limiter √† 10 derniers messages
            if sender == "Vous":
                st.markdown(f"**üë§ {sender}** : {msg}")
            else:
                st.markdown(f"**ü§ñ {sender}** : {msg}")
            if i < len(st.session_state.chat_history) - 1:
                st.markdown("---")

            
            
def main():
    init_session_state()
    
    # Afficher le statut d'Ollama
    show_ollama_status()
    
    # Navigation
    menu = st.sidebar.radio("üß≠ Navigation", ["üì§ Upload", "üìä R√©sultats", "ü§ñ Chatbot"])
    
    if menu == "üì§ Upload":
        st.session_state.current_page = "upload"
    elif menu == "üìä R√©sultats":
        st.session_state.current_page = "results"
    elif menu == "ü§ñ Chatbot":
        st.session_state.current_page = "chatbot"

    # Affichage des pages
    try:
        if st.session_state.current_page == "upload":
            page_upload()
        elif st.session_state.current_page == "results":
            page_results()
        elif st.session_state.current_page == "chatbot":
            page_chatbot()
    except Exception as e:
        st.error(f"‚ùå Erreur dans l'application : {str(e)}")
        st.info("üîÑ Rechargez la page pour recommencer.")
        if st.checkbox("üêõ Mode debug - Afficher les d√©tails"):
            st.exception(e)

if __name__ == "__main__":
    main()